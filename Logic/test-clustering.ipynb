{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1791fbd2-fd3c-4307-8020-abecfcb03cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb12fb52-2302-4bb7-89a0-438e10ed48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class FastTextDataLoader:\n",
    "    \"\"\"\n",
    "    This class is designed to load and pre-process data for training a FastText model.\n",
    "\n",
    "    It takes the file path to a data source containing movie information (synopses, summaries, reviews, titles, genres) as input.\n",
    "    The class provides methods to read the data into a pandas DataFrame, pre-process the text data, and create training data (features and labels)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, preprocess, file_path='data/IMDB_crawled.json'):\n",
    "        \"\"\"\n",
    "        Initializes the FastTextDataLoader class with the file path to the data source.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path: str\n",
    "            The path to the file containing movie information.\n",
    "        \"\"\"\n",
    "        self.preprocess = preprocess\n",
    "        self.file_path = file_path\n",
    "        self.le = None\n",
    "        self.mapping = None\n",
    "\n",
    "    def read_data_to_df(self, should_ignore_empty_genres=True):\n",
    "        \"\"\"\n",
    "        Reads data from the specified file path and creates a pandas DataFrame containing movie information.\n",
    "\n",
    "        You can use an IndexReader class to access the data based on document IDs.\n",
    "        It extracts synopses, summaries, reviews, titles, and genres for each movie.\n",
    "        The extracted data is then stored in a pandas DataFrame with appropriate column names.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "            pd.DataFrame: A pandas DataFrame containing movie information (synopses, summaries, reviews, titles, genres).\n",
    "        \"\"\"\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            documents = json.loads(f.read())\n",
    "            f.close()\n",
    "        data = []\n",
    "        for doc in tqdm(documents):\n",
    "            title = doc.get('title', '')\n",
    "            if title is None:\n",
    "                title = ''\n",
    "            synopsis = doc.get('synopsis', [])\n",
    "            if synopsis is None:\n",
    "                synopsis = []\n",
    "            summaries = doc.get('summaries', [])\n",
    "            if summaries is None:\n",
    "                summaries = []\n",
    "            reviews = doc.get('reviews', [])\n",
    "            if reviews is None:\n",
    "                reviews = []\n",
    "            genres = doc.get('genres', [])\n",
    "            if genres is None:\n",
    "                continue\n",
    "            # Check for empty records\n",
    "            if should_ignore_empty_genres and len(genres) == 0:\n",
    "                print(f'doc_id={doc[\"id\"]} has None genre!')\n",
    "                continue\n",
    "            if title == '' and len(synopsis) == len(summaries) == len(reviews) == 0:\n",
    "                print(f'doc_id={doc[\"id\"]} is None!')\n",
    "                continue\n",
    "            # Preprocess and add to df data\n",
    "            genres = genres[0]\n",
    "            data.append({\n",
    "                'title': self.preprocess(title),\n",
    "                'synopsis': self.preprocess(' '.join(synopsis)),\n",
    "                'summaries': self.preprocess(' '.join(summaries)),\n",
    "                'reviews': self.preprocess(\n",
    "                    ' '.join(x[0] for x in ([['', '']] if reviews is None or len(reviews) == 0 else reviews))),\n",
    "                'genres': self.preprocess(genres),\n",
    "            })\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def create_train_data(self):\n",
    "        \"\"\"\n",
    "        Reads data using the read_data_to_df function, pre-processes the text data, and creates training data (features and labels).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two NumPy arrays: X (preprocessed text data) and y (encoded genre labels).\n",
    "        \"\"\"\n",
    "        df = self.read_data_to_df()\n",
    "        self.le = LabelEncoder()\n",
    "        df['genres'] = self.le.fit_transform(df['genres'])\n",
    "        self.mapping = dict(zip(range(len(self.le.classes_)), self.le.classes_))\n",
    "        df['text'] = df['synopsis'] + ' ' + df['summaries'] + ' ' + df['reviews'] + ' ' + df['title']\n",
    "        x = np.array(df['text'])\n",
    "        y = np.array(df['genres'])\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bed46ff-5204-4f3a-9dad-c1f1c2547d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
    "                    punctuation_removal=True):\n",
    "    \"\"\"\n",
    "    preprocess text by removing stopwords, punctuations, and converting to lowercase, and also filter based on a min length\n",
    "    for stopwords use nltk.corpus.stopwords.words('english')\n",
    "    for punctuations use string.punctuation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        text to be preprocessed\n",
    "    minimum_length: int\n",
    "        minimum length of the token\n",
    "    stopword_removal: bool\n",
    "        whether to remove stopwords\n",
    "    stopwords_domain: list\n",
    "        list of stopwords to be removed base on domain\n",
    "    lower_case: bool\n",
    "        whether to convert to lowercase\n",
    "    punctuation_removal: bool\n",
    "        whether to remove punctuations\n",
    "    \"\"\"\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'<br\\s*/?>', '', text)\n",
    "    text.strip()\n",
    "    if punctuation_removal:\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(stopwords_domain)\n",
    "    if stopword_removal:\n",
    "        new_text = ''\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = pos_tag(word_tokenize(text))\n",
    "        for token in tokens:\n",
    "            word, tag = token\n",
    "            if word not in stopwords_domain and len(word) > minimum_length:\n",
    "                wntag = tag[0].lower()\n",
    "                wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "                if not wntag:\n",
    "                    lemma = word\n",
    "                else:\n",
    "                    lemma = lemmatizer.lemmatize(word, wntag)\n",
    "\n",
    "                new_text = new_text + lemma + ' '\n",
    "        text = new_text\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "class FastText:\n",
    "    \"\"\"\n",
    "    A class used to train a FastText model and generate embeddings for text data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    method : str\n",
    "        The training method for the FastText model.\n",
    "    model : fasttext.FastText._FastText\n",
    "        The trained FastText model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='skipgram'):\n",
    "        \"\"\"\n",
    "        Initializes the FastText with a preprocessor and a training method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str, optional\n",
    "            The training method for the FastText model.\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, texts, text_file_path='data/FastText_data.txt', should_load_data=False):\n",
    "        \"\"\"\n",
    "        Trains the FastText model with the given texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        texts : list of str\n",
    "            The texts to train the FastText model.\n",
    "        \"\"\"\n",
    "        if should_load_data:\n",
    "            all_text = ''\n",
    "            for text in tqdm(texts):\n",
    "                all_text += text + '\\n'\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(all_text)\n",
    "                file.close()\n",
    "\n",
    "        self.model = fasttext.train_unsupervised(text_file_path, model=self.method)\n",
    "        print(\"Model trained successfully\")\n",
    "\n",
    "    def get_query_embedding(self, query):\n",
    "        \"\"\"\n",
    "        Generates an embedding for the given query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            The query to generate an embedding for.\n",
    "        tf_idf_vectorizer : sklearn.feature_extraction.text.TfidfVectorizer\n",
    "            The TfidfVectorizer to transform the query.\n",
    "        do_preprocess : bool, optional\n",
    "            Whether to preprocess the query.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The embedding for the query.\n",
    "        \"\"\"\n",
    "        preprocessed_query = preprocess_text(query)\n",
    "        return self.model.get_sentence_vector(preprocessed_query)\n",
    "\n",
    "    def analogy(self, word1, word2, word3):\n",
    "        \"\"\"\n",
    "        Perform an analogy task: word1 is to word2 as word3 is to __.\n",
    "\n",
    "        Args:\n",
    "            word1 (str): The first word in the analogy.\n",
    "            word2 (str): The second word in the analogy.\n",
    "            word3 (str): The third word in the analogy.\n",
    "\n",
    "        Returns:\n",
    "            str: The word that completes the analogy.\n",
    "        \"\"\"\n",
    "        # Obtain word embeddings for the words in the analogy\n",
    "        embedding1 = self.model[word1]\n",
    "        embedding2 = self.model[word2]\n",
    "        embedding3 = self.model[word3]\n",
    "\n",
    "        # Perform vector arithmetic\n",
    "        v = embedding3 + embedding2 - embedding1\n",
    "\n",
    "        # Create a dictionary mapping each word in the vocabulary to its corresponding vector\n",
    "        words = list(self.model.words.copy())\n",
    "\n",
    "        # Exclude the input words from the possible results\n",
    "        words = list(set(words).difference([word1, word2, word3]))\n",
    "\n",
    "        # Find the word whose vector is closest to the result vector\n",
    "        c_score = math.inf\n",
    "        chosen_vector = None\n",
    "        for word in words:\n",
    "            score = distance.cosine(v, self.model[word])\n",
    "            if score < c_score:\n",
    "                c_score = score\n",
    "                chosen_vector = word\n",
    "        return chosen_vector\n",
    "\n",
    "    def save_model(self, path='data/FastText_model.bin'):\n",
    "        \"\"\"\n",
    "        Saves the FastText model to a file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, optional\n",
    "            The path to save the FastText model.\n",
    "        \"\"\"\n",
    "        self.model.save_model(path)\n",
    "\n",
    "    def load_model(self, path=\"data/FastText_model.bin\"):\n",
    "        \"\"\"\n",
    "        Loads the FastText model from a file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, optional\n",
    "            The path to load the FastText model.\n",
    "        \"\"\"\n",
    "        self.model = fasttext.load_model(path)\n",
    "\n",
    "    def prepare(self, dataset, mode, save=False, path='/Users/divar/University/term-8/information-retrieval/imdb-mir-system/Logic/data/FastText_model.bin'):\n",
    "        \"\"\"\n",
    "        Prepares the FastText model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : list of str\n",
    "            The dataset to train the FastText model.\n",
    "        mode : str\n",
    "            The mode to prepare the FastText model.\n",
    "        \"\"\"\n",
    "        if mode == 'train':\n",
    "            self.train(dataset)\n",
    "        if mode == 'load':\n",
    "            self.load_model(path)\n",
    "        if save:\n",
    "            self.save_model(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bd585c-68d4-48c7-bdf1-e09cbb9cbf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████████▍                                                                                                                                        | 705/9950 [01:44<10:21, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt6731210 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████████▊                                                                                                                               | 1279/9950 [02:44<11:33, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt26908364 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████████████▋                                                                                                                           | 1543/9950 [03:14<12:28, 11.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt17497130 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████████████▏                                                                                                                          | 1583/9950 [03:19<14:41,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt15799564 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████████▉                                                                                                                      | 1904/9950 [03:54<12:34, 10.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt14299894 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████████████████████████████▎                                                                                                                  | 2132/9950 [04:18<10:15, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt0251123 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████████████████████████▊                                                                                                   | 3192/9950 [05:50<04:13, 26.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt23765492 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████████████████████████                                                                                                   | 3206/9950 [05:50<03:31, 31.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt4432124 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████████████████████                                                                                               | 3482/9950 [06:17<11:46,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt31637517 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████████████████████████████▌                                                                                         | 3856/9950 [06:50<07:04, 14.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt12963502 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|█████████████████████████████████████████████████████████▏                                                                                        | 3901/9950 [06:54<08:10, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt29768342 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████▊                                                                             | 4689/9950 [08:11<06:10, 14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt29867105 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████████████████▊                                                                            | 4758/9950 [08:17<06:49, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt27140032 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████████████████████████████████████████████▊                                                           | 5916/9950 [09:52<06:16, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt31107449 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████▉                                                           | 5929/9950 [09:53<04:43, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt14962296 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 6037/9950 [10:04<03:37, 17.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt7985982 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 7221/9950 [11:42<01:01, 44.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt14549284 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 7528/9950 [12:08<02:46, 14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt1546032 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                            | 7984/9950 [12:45<01:28, 22.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt31181287 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 8016/9950 [12:48<02:01, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt14223750 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                          | 8176/9950 [13:00<00:22, 79.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt24950660 has None genre!\n",
      "doc_id=tt31183803 has None genre!\n",
      "doc_id=tt31123081 has None genre!\n",
      "doc_id=tt21418340 has None genre!\n",
      "doc_id=tt20215356 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                    | 8573/9950 [13:27<01:41, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt27863908 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 8733/9950 [13:40<00:42, 28.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt15073568 has None genre!\n",
      "doc_id=tt31450459 has None genre!\n",
      "doc_id=tt23783950 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                | 8846/9950 [13:49<00:42, 25.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt11481690 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 9348/9950 [14:22<00:23, 25.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt0199066 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████     | 9615/9950 [14:43<00:10, 30.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt28529522 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 9837/9950 [14:55<00:03, 33.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt30982784 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 9862/9950 [14:57<00:05, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=tt0319762 has None genre!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9950/9950 [15:04<00:00, 11.00it/s]\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "dataloader = FastTextDataLoader(preprocess_text)\n",
    "X, y = dataloader.create_train_data()\n",
    "document_labels = list(dataloader.le.inverse_transform(y))\n",
    "ft_model = FastText()\n",
    "ft_model.prepare(dataset=None, mode='load', save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b0951c-894f-4e35-b5df-c125bdef13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9916/9916 [14:53<00:00, 11.10it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "with tqdm(X) as pbar:\n",
    "    for x in pbar:\n",
    "        embeddings.append(ft_model.get_query_embedding(x))\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c532305b-f336-44ec-8192-53df2b228fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/divar/University/term-8/information-retrieval/imdb-mir-system/Logic/data/clustering'\n",
    "np.save(f'{base_path}/embeddings.npy', embeddings)\n",
    "np.save(f'{base_path}/labels.npy', y)\n",
    "with open(f'{base_path}/document_labels.json', 'w') as f:\n",
    "    f.write(json.dumps(document_labels))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5109d-edaa-44d9-a701-76f12daa392d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
